# RAG技术概览

> https://zhuanlan.zhihu.com/p/678893732


检索增强生成（Retrieval Augmented Generation，简称RAG）

## 基础RAG

我认为的基础RAG是以一系列文本资料作为起点。 当然在这之前还有一些数据获取的步骤，不过这些我认为他们是独立与RAG之外的部分。

简单描述一下就是将一系列文本资料作为数据源（Database） ， 在接收到一个用户提问（query）的时候，在Database中查找相关联的资料文本，然后组合成一个向大模型（LLM） 提问的 prompt，最后拿到LLM的输出（answer）

当然这个从Database中查找相关联资料的过程一般会用到文本嵌入模型（embedding），从Database中根据向量相似度查找

示意流程图
``` mermaid
graph LR
    A(query) --> B([embedding]) --> C[向量化数据] --通过向量相似度算法检索--> D[(Database)]  

    --> E(组装prompt)  --> F([LLM]) --> G(answer)

```

这个Database是我们提前初始化过的，通过embedding模型对数据（分块数据chunks）进行向量化索引
用户的提问，同样进行一下向量化，通过向量相似度算法（如欧式距离或者余弦相似度）从Database中查询相关的数据块
获取到数据块之后就需要组装prompt，下面是一个适用于openAI风格模型的prompt示例：

system prompt 系统提示词
``` 
你是智慧助手，专注于通过分析和理解用户的问题`question`，从用户提供的`content`获取准确的答案。

## 目标
- 确保响应的相关性和准确性。
- 根据用户的提问`question`，从`content`中提取最相关的信息以提供帮助。
- 在回答中保持客观、中立和基于事实的态度。

## 使用说明
1. 当用户提出问题`question`时，首先明确其意图。
2. 分析`content`，从中找出与用户问题`question`最相关的部分。
3. 以清晰、简洁的方式组织答案，并确保信息的准确性。
4. 如果`content`中没有直接相关的答案，请给出可能的建议或方向，并告知用户。

## 示例流程
- 用户询问：“question:如何在家制作披萨？
           content: [一些关于披萨制作的知识] ”
- 助手分析`content`中关于披萨制作的部分，然后回答：
    - “在家制作披萨的基本步骤包括准备面团、添加酱料和配料，最后烘烤。具体来说，首先将面粉、酵母、水和油混合制成面团，放置一段时间让其发酵。接着，在面团上涂抹番茄酱，并根据个人口味加入奶酪和其他喜欢的配料，如蔬菜、肉类等。最后，在预热至450华氏度（约230摄氏度）的烤箱中烤制10到15分钟。”

请记住，您的回答应该基于`content`中提供的信息，并尽可能满足用户的需求。如果遇到不清楚或敏感的问题，请谨慎处理，避免提供不准确或潜在有害的回答。


```
user prompt 用户提示词
```
question:
{{}}


content:
{{content}}

```


其中 system prompt作为 system角色的输入，user prompt作为user 角色的输入
当然也可以整合成为一条，将system prompt 和 user prompt结合，在一个prompt中设定输入输出格式要求，遵循何种规范等


## 高级RAG

高级RAG实际是在基础RAG上的增强，对基础RAG的各个步骤进行增强，从而提升总体性能

将基础RAG每个步骤拆分开，可以大致对三个部分进行增强

### 检索增强

#### 文本分块和向量化

建立一个向量索引，这个索引代表我们的资料内容，查询时查找这些向量中与查询向量的最大余弦相似度，获取语义上最相近的N的分块文本

##### 为什么要分块
大模型（transformer模型）的输入序列长度是固定的，即使输入上下文窗口很大，也有一个上限，所以需要用一个或几个句子来代表他们的语义含义。
对数据进行分块处理，将文档分为大小合适的段落，比如每一个句子分块，或者每一个段落分块。
具体的分块大小取决于嵌入模型以及大语言模型能够处理的能力。
##### 向量化
选择一个适合的模型来向量化这些文本块，以获取最佳的查询效果


#### 搜索索引

##### 向量存储索引

就向其他类型的数据字段一样，向量数据也可以添加索引对其查询进行优化

- 扁平索引 （暴力计算）
    不会对输入向量做处理，通过计算与库中所有向量的相似度计算获取前k个结果（kNN算法）
    准确性最高，但代价是最长搜索时间

    优化策略：
    - 向量降维-> 减小向量大小
    - 向量聚类，然后限制搜索范围为最接近的聚类中  -> 减少搜索范围

- 倒排文件索引（IVF）
    通过聚类来缩小查询范围
- 分层可导航小世界（HNSW）
    在向量数据点之间构建一个分层图机构，每一层都连接着一部分数据点，通过这种方式，优化了最近邻搜索的效率
    工作原理：
      - 图结构： 构建多个图层来提高搜索效率，每一层图的节点数量和连接方式都可以进行调整
        - 顶层图：包含少量节点，每个节点与其他节点的连接较为稀疏
        - 底层图：包含大量节点，每个节点与邻近节点的连接较多
      - 层次结构： 每个数据点在不同层次上都有不同的“表示”。层次越高节点越少，搜索范围越广。数据根据一定概率分配到不同层级，高层节点少，地层节点多。
        - 高层： 每个节点连接较少，用于大范围搜索。快速过滤不相干的区域
        - 底层： 每个节点的连接较多，负责精确地搜索最近邻。
      - 导航过程： 从最顶层图开始，通过一系列连接跳跃，逐步向下深入，最终寻找到目标节点的最近节点
        - 在顶层图找到最近的节点
        - 根据这个节点跳转到下一层，再一次查找最近节点
        - 直到底层图，找到最近的邻居 

##### 假设性问题、假设性回答

- 假设性问题： 让LLM为每个块生成一个假设性的问题，并将这些问题向量化嵌入。运行时对这些问题向量进行查询搜索（根据用户问题查询假设问题），检索后将原始文本作为上下文发送给LLM获取答案。
由于假设性问题一般与用户问题相似度更高，能提高搜索质量

- 假设性回答（HyDE）：在运行时让LLM根据用户问题生成一个假设性的回答，结合这个回答的向量与
查询向量一起搜索，提高搜索质量


##### 上下文增强
通过减少较小的块、提高搜索质量，但是同时增加周围的上下文供LLM推理分析。

- 句子窗口检索
  将文档中每个句子（或段落）单独嵌入向量，提高搜索质量
  在检索到关键句子后，前后扩展K个句子，将这个扩展后的上下文发送给LLM
- 父文档检索
  先将文档分割为较小的子块，这些子块与更大父块相对应（这里可以多级分块）
  检索中检索出K个子块，如果其中n块都对应同一个父块，那么就用这个父块来代替这些子块，作为扩展后的上下文输入LLM


##### 混合搜索
结合两种不同的搜索算法
- 基于关键词的传统搜索方法 使用稀疏检索算法（tf-idf、BM25等）
- 语义或向量搜索
结合这两种搜索，将他们的检索结果融合产生一个综合的检索结果。

这里的关键是正确的结合具有不同相似度评分的检索结构，通常通过RRF（倒数排序融合）算法解决，
该算法会对检索结果进行重新排名，以产生最终输出。

混合或融合搜索通常能提供更好的检索结果，因为它结合了两种互补的搜索算法，既考虑了查询与存储文档之间的语义相似性，也考虑了关键词的匹配


#### 重排名和过滤

使用上面的任何一种算法得到了检索结果，后续需要通过过滤、重新排名或者进行一些转换进一步优化这些结果。

#### 查询转换
查询转换是一组通过LLM作为推理引擎来修改用户输入，以此提高检索质量的技术。

如果查询问题复杂，可以通过LLM将其分解为几个子查询
例如： 
    用户提问：Apache2.0协议和MIT协议哪个使用人数更多
    分解为：
    - Apache2.0协议的使用人数大概是多少
    - MIT协议的使用人数大概是多少

分解后的查询将被并行执行检索，然后将检索到的上下文组合到一个提示中，给LLM综合出对初始查询的最终答案。

- 回退式查询： 使用LLM生成更一般的查询，以检索获取更一般或全面的上下文，结合原始查询的检索结果，结合两个上下文输入LLM
- 查询重写，使用LLM重新构造初始查询以提高检索效果


#### 添加引用
如果使用了多个来源来生成答案，不论是因为初始查询太复杂（需要通过多个子查询结果融合），或者是在不同文档都查询到了相关上下文，都有一个问题，需要反向引用我们的来源

有几个方法:
- 将这个引用任务插入到提示中，并要求LLM提及所使用的来源的ID
- 将生成的响应部分与索引中的原始文本块匹配
  - 模糊匹配 （llamaIndex 提供了一个FuzzyCitationEnginePack 高效匹配引用）



#### 对话引擎

聊天逻辑的开发十分关键，需要考虑多种因素，如是否需要对话上下文、模型输入窗口大小等等。
一般为了提高对单个搜索查询的反复有效工作，需要考虑对话的上下文，类似以前的聊天机器人，为了处理上下文中的指代或者与之前对话相关的
任意指令，一般会采取查询压缩技术，这种技术在处理用户查询的同时考虑了聊天的上下文。

关于上下文的压缩，有这些方法：
- 上下文对话引擎（参考LlamaIndex的Chat Engine - Context Mode）：  每次聊天交互时
  1. 首先从索引中检索用户的消息文本，作为系统提示的上下文（system prompt）
  2. 将对话历史和系统提示一起发送到LLM
  3. 返回用户消息的答案 
- 精简上下文对话引擎（参考LlamaIndex的Chat Engine -  Condense Plus Context Mode）： 每次聊天交互时
  1. 首先将对话历史和最新的用户消息压缩成一个独立的问题（借助LLM）
  2. 然后根据这个独立问题从索引中检索，构建上下文
  3. 将上下文、系统提示、用户消息一起传递给LLM生成响应
- 还有其他的对话引擎，可以参考[LlamaIndex Chat Engine](https://docs.llamaindex.ai/en/stable/examples/chat_engine/chat_engine_best/) 


#### 查询路由

查询路由是基于LLM的决策步骤，用来确定针对用户查询接下来应采取的行动
如：概括回答、执行数据搜索、多路查询、结果合成等等

查询路由一般可以用于选择合适的索引或合适的数据源来处理用户查询，比如根据用户查询选择对应的主题数据源
或者是选择关键词索引（基于传统搜索技术）还是向量索引，再或者是选择多层次的索引数据源等

设置路由器涉及的、可以确定的路由选择，具体的路由选择通过LLM调用来实现，通过prompt预定义返回的结果格式
据此来执行具体的行动，比如选择具体的索引或者调用其他智能体等等



### 输出增强

#### RAG智能体


#### 响应合成


### 模型增强


### 评估

RAG 系统的性能评估通常涉及多个独立的指标，包括整体答案的相关性、答案的根据性、信实度以及检索到的上下文的相关性等。
可以通过信实度和答案相关性作为评估生成答案质量的指标，如传统的上下文精确度和召回率来评估检索效果。

另外还可以通过下面三个维度来评估：
1. 检索到的上下文与查询的相关性
2. 答案的根据性（大语言模型的答案在多大程度上得到提供的上下文的支持
3. 答案对查询的相关性
其中最直观也最容易改变就是1 上下文与查询的相关性，检索增强中提到的那些技术，几乎都是为了提高这一项指标






